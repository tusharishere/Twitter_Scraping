{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29d18e0a-6f3a-45f9-9396-dcec9ef62e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|███████████████████████████████████████████████████████████████████████           | 13/15 [01:52<00:24, 12.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timed out waiting for AETechnologies1 page to load.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 15/15 [01:58<00:00,  7.91s/it]\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "\n",
    "def scrape_twitter_profiles(profiles):\n",
    "    # Set up the Chrome WebDriver\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.maximize_window()\n",
    "\n",
    "    # Create CSV file and write header\n",
    "    with open('twitter_profiles.csv', 'w', newline='', encoding='utf-8') as csv_file:\n",
    "        csv_writer = csv.writer(csv_file)\n",
    "        csv_writer.writerow(['Profile', 'Bio', 'Following Count', 'Followers Count', 'Location', 'Website'])\n",
    "\n",
    "        # Iterate through the Twitter profiles\n",
    "        for profile in tqdm(profiles):\n",
    "            # Visit the Twitter profile\n",
    "            driver.get(f'https://twitter.com/{profile}')\n",
    "            \n",
    "            # Use WebDriverWait to wait for elements to be present before interacting with them\n",
    "            try:\n",
    "                WebDriverWait(driver, 20).until(\n",
    "                    EC.presence_of_element_located((By.XPATH, '//div[@data-testid=\"UserDescription\"]'))\n",
    "                )\n",
    "            except TimeoutException:\n",
    "                print(f\"Timed out waiting for {profile} page to load.\")\n",
    "                continue\n",
    "\n",
    "            # Extract profile information with WebDriverWait\n",
    "            try:\n",
    "                bio = WebDriverWait(driver, 10).until(\n",
    "                    EC.presence_of_element_located((By.XPATH, '//div[@data-testid=\"UserDescription\"]'))\n",
    "                ).text\n",
    "            except TimeoutException:\n",
    "                bio = 'Not available'\n",
    "\n",
    "            try:\n",
    "                following_count = WebDriverWait(driver, 10).until(\n",
    "                    EC.presence_of_element_located((By.XPATH, '(//a[@role=\"link\"]/span/span)[1]'))\n",
    "                ).text\n",
    "            except TimeoutException:\n",
    "                following_count = '0'\n",
    "\n",
    "            try:\n",
    "                followers_count = WebDriverWait(driver, 10).until(\n",
    "                    EC.presence_of_element_located((By.XPATH, '(//a[@role=\"link\"]/span/span)[3]'))\n",
    "                ).text\n",
    "            except TimeoutException:\n",
    "                followers_count = '0'\n",
    "\n",
    "            try:\n",
    "                location = WebDriverWait(driver, 10).until(\n",
    "                    EC.presence_of_element_located((By.XPATH, '//span[@data-testid=\"UserLocation\"]/span/span'))\n",
    "                ).text\n",
    "            except TimeoutException:\n",
    "                location = 'Not available'\n",
    "\n",
    "            try:\n",
    "                website = WebDriverWait(driver, 10).until(\n",
    "                    EC.presence_of_element_located((By.XPATH, '//a[@data-testid=\"UserUrl\"]/span'))\n",
    "                ).text\n",
    "            except TimeoutException:\n",
    "                website = 'Not available'\n",
    "\n",
    "            # Write data to CSV\n",
    "            csv_writer.writerow([profile, bio, following_count, followers_count, location, website])\n",
    "\n",
    "    # Close the browser\n",
    "    driver.quit()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # List of Twitter profiles to scrape\n",
    "    twitter_profiles = ['GTNUK1', 'whatsapp', 'aacb_CBPTrade', 'aacbdotcom', 'AAWindowPRODUCT',\n",
    "                        'aandb_kia', 'ABHomeInc', 'Abrepro', 'endangeredprani', 'ACChristofiLtd',\n",
    "                        'aeclothing1', 'mufaddal_vohra', 'AETechnologies1', 'wix', 'AGInsuranceLLC']\n",
    "\n",
    "    # Call the function to scrape profiles\n",
    "    scrape_twitter_profiles(twitter_profiles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9e8b7d-9963-4497-b4ab-fb3e900e7db5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
